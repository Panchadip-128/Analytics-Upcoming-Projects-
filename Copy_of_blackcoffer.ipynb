{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToS2jqaP8WMP",
        "outputId": "6f219ca3-9b37-48fe-dd19-511f23976d6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 nltk pandas openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcwm_qJ_8Znv",
        "outputId": "8b024ac2-1fcb-44a2-dafb-4af9934da013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jupyter\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.5.5)\n",
            "Collecting qtconsole (from jupyter)\n",
            "  Downloading qtconsole-5.5.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter) (7.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (6.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter) (3.0.11)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (5.10.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (1.3.0)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (1.1.0)\n",
            "Collecting qtpy>=2.4.0 (from qtconsole->jupyter)\n",
            "  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyter)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->jupyter) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter) (4.19.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter) (0.18.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.2.1)\n",
            "Installing collected packages: qtpy, jedi, qtconsole, jupyter\n",
            "Successfully installed jedi-0.19.1 jupyter-1.0.0 qtconsole-5.5.2 qtpy-2.4.1\n"
          ]
        }
      ],
      "source": [
        "pip install jupyter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXVBaEVc3bRA",
        "outputId": "261b672f-6444-443f-b8e3-99422c3517be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles extracted and saved.\n"
          ]
        }
      ],
      "source": [
        "# Installing necessary libraries\n",
        "!pip install requests beautifulsoup4 nltk pandas openpyxl\n",
        "\n",
        "# Importing libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import cmudict\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Downloading nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Loading the input Excel file\n",
        "input_file = '/content/Input.xlsx'\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Creating a directory to save the articles\n",
        "if not os.path.exists('articles'):\n",
        "    os.makedirs('articles')\n",
        "\n",
        "# Function to extract article text from a URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        title = soup.find('h1')\n",
        "        if title:\n",
        "            title_text = title.get_text()\n",
        "        else:\n",
        "            title_text = ''\n",
        "\n",
        "        paragraphs = soup.find_all('p')\n",
        "        if not paragraphs:\n",
        "            print(f\"No paragraphs found in the article from {url}\")\n",
        "            return title_text\n",
        "\n",
        "        article_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        return title_text + '\\n' + article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract article from {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Extracting and saving articles\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    article_text = extract_article_text(url)\n",
        "    with open(f'articles/{url_id}.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(article_text)\n",
        "\n",
        "print(\"Articles extracted and saved.\")\n",
        "\n",
        "# Loading positive and negative words for sentiment analysis\n",
        "positive_words = set(open('/content/positive-words.txt', encoding='latin1').read().split())\n",
        "negative_words = set(open('/content/negative-words.txt', encoding='latin1').read().split())\n",
        "\n",
        "# Helper functions\n",
        "def syllable_count(word):\n",
        "    d = cmudict.dict()\n",
        "    try:\n",
        "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
        "    except KeyError:\n",
        "        return 0\n",
        "\n",
        "def complex_word(word):\n",
        "    return syllable_count(word) > 2\n",
        "\n",
        "def text_analysis(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    word_count = len(words)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    positive_score = sum(1 for word in words if word in positive_words)\n",
        "    negative_score = sum(1 for word in words if word in negative_words)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "    complex_words = [word for word in words if complex_word(word)]\n",
        "    percentage_complex_words = len(complex_words) / word_count if word_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    syllables_per_word = sum(syllable_count(word) for word in words) / word_count if word_count else 0\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "    avg_word_length = sum(len(word) for word in words) / word_count if word_count else 0\n",
        "\n",
        "    return {\n",
        "        'Positive Score': positive_score,\n",
        "        'Negative Score': negative_score,\n",
        "        'Polarity Score': polarity_score,\n",
        "        'Subjectivity Score': subjectivity_score,\n",
        "        'Avg Sentence Length': avg_sentence_length,\n",
        "        'Percentage of Complex Words': percentage_complex_words,\n",
        "        'Fog Index': fog_index,\n",
        "        'Avg Number of Words Per Sentence': avg_sentence_length,\n",
        "        'Complex Word Count': len(complex_words),\n",
        "        'Word Count': word_count,\n",
        "        'Syllable Per Word': syllables_per_word,\n",
        "        'Personal Pronouns': personal_pronouns,\n",
        "        'Avg Word Length': avg_word_length\n",
        "    }\n",
        "\n",
        "# Load the articles and perform analysis\n",
        "articles_dir = 'articles'\n",
        "output_data = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    file_path = os.path.join(articles_dir, f'{url_id}.txt')\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    analysis_results = text_analysis(text)\n",
        "    row_data = {**row, **analysis_results}\n",
        "    output_data.append(row_data)\n",
        "\n",
        "# Saving the results to an Excel file\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_excel('Output Data Structure.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis completed and saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install requests beautifulsoup4 nltk pandas openpyxl\n",
        "\n",
        "# Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import cmudict\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Download nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the input Excel file\n",
        "input_file = '/content/Input.xlsx'\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Create a directory to save the articles\n",
        "if not os.path.exists('articles'):\n",
        "    os.makedirs('articles')\n",
        "\n",
        "# Function to extract article text from a URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        title = soup.find('h1')\n",
        "        if title:\n",
        "            title_text = title.get_text()\n",
        "        else:\n",
        "            title_text = ''\n",
        "\n",
        "        paragraphs = soup.find_all('p')\n",
        "        if not paragraphs:\n",
        "            print(f\"No paragraphs found in the article from {url}\")\n",
        "            return title_text\n",
        "\n",
        "        article_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        return title_text + '\\n' + article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract article from {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Extract and save articles\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    article_text = extract_article_text(url)\n",
        "    with open(f'articles/{url_id}.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(article_text)\n",
        "\n",
        "print(\"Articles extracted and saved.\")\n",
        "\n",
        "# Load positive and negative words for sentiment analysis\n",
        "positive_words = set(open('/content/positive-words.txt', encoding='latin1').read().split())\n",
        "negative_words = set(open('/content/negative-words.txt', encoding='latin1').read().split())\n",
        "\n",
        "# Helper functions\n",
        "def syllable_count(word):\n",
        "    d = cmudict.dict()\n",
        "    try:\n",
        "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
        "    except KeyError:\n",
        "        return 0\n",
        "\n",
        "def complex_word(word):\n",
        "    return syllable_count(word) > 2\n",
        "\n",
        "def text_analysis(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    word_count = len(words)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    positive_score = sum(1 for word in words if word in positive_words)\n",
        "    negative_score = sum(1 for word in words if word in negative_words)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "    complex_words = [word for word in words if complex_word(word)]\n",
        "    percentage_complex_words = len(complex_words) / word_count if word_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    syllables_per_word = sum(syllable_count(word) for word in words) / word_count if word_count else 0\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "    avg_word_length = sum(len(word) for word in words) / word_count if word_count else 0\n",
        "\n",
        "    return {\n",
        "        'Positive Score': positive_score,\n",
        "        'Negative Score': negative_score,\n",
        "        'Polarity Score': polarity_score,\n",
        "        'Subjectivity Score': subjectivity_score,\n",
        "        'Avg Sentence Length': avg_sentence_length,\n",
        "        'Percentage of Complex Words': percentage_complex_words,\n",
        "        'Fog Index': fog_index,\n",
        "        'Avg Number of Words Per Sentence': avg_sentence_length,\n",
        "        'Complex Word Count': len(complex_words),\n",
        "        'Word Count': word_count,\n",
        "        'Syllable Per Word': syllables_per_word,\n",
        "        'Personal Pronouns': personal_pronouns,\n",
        "        'Avg Word Length': avg_word_length\n",
        "    }\n",
        "\n",
        "# Load the articles and perform analysis\n",
        "articles_dir = 'articles'\n",
        "output_data = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    file_path = os.path.join(articles_dir, f'{url_id}.txt')\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    analysis_results = text_analysis(text)\n",
        "    row_data = {**row, **analysis_results}\n",
        "    output_data.append(row_data)\n",
        "\n",
        "# Save the results to an Excel file\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_excel('Output Data Structure.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis completed and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw7pc0GFbi5o",
        "outputId": "6e19da96-5473-44ec-c8a9-3fd1b308667a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles extracted and saved.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}